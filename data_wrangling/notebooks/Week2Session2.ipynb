{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee56b003",
   "metadata": {},
   "source": [
    "## Data Wrangling II:\n",
    "### 1. Data Preparation\n",
    "### 2. Encoding\n",
    "### 3. Extra Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ceb831",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9e307",
   "metadata": {},
   "source": [
    "### 1.1 Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7382ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387447c4",
   "metadata": {},
   "source": [
    "For data with float64 dtype, pandas uses the floating-point value `NaN` (Not a Number) to represent missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda4cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data = pd.Series([1.2, -3.5, np.nan, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f44334",
   "metadata": {},
   "source": [
    "The `isna` method gives us a Boolean Series with True where values are null:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc277c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f19d20",
   "metadata": {},
   "source": [
    "The built-in Python None value is also treated as NA:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe87af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data = pd.Series([\"aardvark\", np.nan, None, \"avocado\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b3da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_data.isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a5d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data = pd.Series([1, 2, None], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a717408",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e347ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_data.isna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f86e2",
   "metadata": {},
   "source": [
    "There are a few ways to filter out missing data. While you always have the option to do it by hand using pandas `.isna` and Boolean indexing, `dropna` can be helpful. On a Series, it returns the Series with only the nonnull data and index values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864dca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1, np.nan, 3.5, np.nan, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5870cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4093a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341a14c",
   "metadata": {},
   "source": [
    "With DataFrame objects, there are different ways to remove missing data. You may want to drop rows or columns that are all `NA`, or only those rows or columns containing any `NA`s at all. `dropna` by default drops any row containing a missing value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c6bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n",
    "                     [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9710ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42df1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210d351",
   "metadata": {},
   "source": [
    "Passing `how=\"all\"` will drop only rows that are all `NA`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d77de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bead7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep in mind that these functions return new objects by default and do not modify the contents \n",
    "# of the original object.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b00e2e",
   "metadata": {},
   "source": [
    "To drop columns in the same way, pass `axis=\"columns\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[4] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87358ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(axis=\"columns\", how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60537fc",
   "metadata": {},
   "source": [
    "Suppose you want to keep only rows containing at most a certain number of missing observations. You can indicate this with the thresh argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.standard_normal((7, 3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346768aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:4, 1] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b3be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:2, 2] = np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda3ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd9fe7",
   "metadata": {},
   "source": [
    "Calling fillna with a constant replaces missing values with a certain value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9dc6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da4e295",
   "metadata": {},
   "source": [
    "The same interpolation methods available for reindexing can be used with fillna:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a45cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.standard_normal((6, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac487a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2:, 1] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbc1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4:, 2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659e113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eee1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(method=\"ffill\", limit=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e392b0c",
   "metadata": {},
   "source": [
    "With fillna you can do other things such as simple data imputation using the median or mean statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ab7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., np.nan, 3.5, np.nan, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ae1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bb742",
   "metadata": {},
   "source": [
    "### 1.2 Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559b0caf",
   "metadata": {},
   "source": [
    "#### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b87dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"k1\": [\"one\", \"two\"] * 3 + [\"two\"],\n",
    "                         \"k2\": [1, 1, 2, 3, 3, 4, 4]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24118c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a9d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8135e6b6",
   "metadata": {},
   "source": [
    "`drop_duplicates` returns a DataFrame with rows where the duplicated array is `False` filtered out:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ca417",
   "metadata": {},
   "source": [
    "Both methods by default consider all of the columns; alternatively, you can specify any subset of them to detect duplicates. Suppose we had an additional column of values and wanted to filter duplicates based only on the \"k1\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5256a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"v1\"] = range(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b05eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba9086",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=[\"k1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f30cdd3",
   "metadata": {},
   "source": [
    "`duplicated` and `drop_duplicates` by default keep the first observed value combination. Passing `keep=\"last\"` will return the last one:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9981539",
   "metadata": {},
   "source": [
    "#### Transforming data with functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a14940",
   "metadata": {},
   "source": [
    "Consider the following data collected about various kinds of meat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({\"food\": [\"bacon\", \"pulled pork\", \"bacon\",\n",
    "                              \"pastrami\", \"corned beef\", \"bacon\",\n",
    "                              \"pastrami\", \"honey ham\", \"nova lox\"],\n",
    "                     \"ounces\": [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3885aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545d70e",
   "metadata": {},
   "source": [
    "Suppose you wanted to add a column indicating the type of animal that each food came from. Let’s write down a mapping of each distinct meat type to the kind of animal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ca973",
   "metadata": {},
   "outputs": [],
   "source": [
    "meat_to_animal = {\n",
    "  \"bacon\": \"pig\",\n",
    "  \"pulled pork\": \"pig\",\n",
    "  \"pastrami\": \"cow\",\n",
    "  \"corned beef\": \"cow\",\n",
    "  \"honey ham\": \"pig\",\n",
    "  \"nova lox\": \"salmon\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4003f",
   "metadata": {},
   "source": [
    "The `map` method on a Series accepts a function or dictionary-like object containing a mapping to do the transformation of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ec60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"animal\"] = data[\"food\"].map(meat_to_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e87f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e111a6e",
   "metadata": {},
   "source": [
    "We could also have passed a function that does all the work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10149568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_animal(x):\n",
    "    return meat_to_animal[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbb7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"food\"].map(get_animal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef771f",
   "metadata": {},
   "source": [
    "#### Replacing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e842188",
   "metadata": {},
   "source": [
    "Filling in missing data with the `fillna` method is a special case of more general value replacement. `map` can be used to modify a subset of values in an object, but replace provides a simpler and more flexible way to do so. Let’s consider this Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96235db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series([1., -999., 2., -999., -1000., 3.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1821acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36064c16",
   "metadata": {},
   "source": [
    "The `-999` values might be an indicator of values for missing data. To replace these with NA values that pandas understands, we can use replace, producing a new Series:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9016e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(-999, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a1f794",
   "metadata": {},
   "source": [
    "If you want to replace multiple values at once, you instead pass a list and then the substitute value:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099992a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([-999, -1000], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c764675",
   "metadata": {},
   "source": [
    "To use a different replacement for each value, pass a list of substitutes:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f68e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace([-999, -1000], [np.nan, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f859e524",
   "metadata": {},
   "source": [
    "The argument passed can also be a dictionary:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ff66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace({-999: np.nan, -1000: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd50d0",
   "metadata": {},
   "source": [
    "## 2. Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413137c",
   "metadata": {},
   "source": [
    "### 2.1 Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad72fa",
   "metadata": {},
   "source": [
    "The first case of categories is about data discretization. Continuous data is often discretized or otherwise separated into “bins” for analysis. Suppose you have data about a group of people in a study, and you want to group them into discrete age buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35771265",
   "metadata": {},
   "source": [
    "Let’s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older. To do so, you have to use `pandas.cut`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fd5bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [18, 25, 35, 60, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261dad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_categories = pd.cut(ages, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161c2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2d5c56",
   "metadata": {},
   "source": [
    "The object pandas returns is a special Categorical object. The output you see describes the bins computed by `pandas.cut`. Each bin is identified by a special (unique to pandas) interval value type containing the lower and upper limit of each bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ab796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_categories.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a334e",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_categories.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fde6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_categories.categories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e9eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e409a2",
   "metadata": {},
   "source": [
    "`pd.value_counts(categories)` are the bin counts for the result of `pandas.cut`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4bdd64",
   "metadata": {},
   "source": [
    "### 2.2 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425db8f5",
   "metadata": {},
   "source": [
    "Another type of transformation for statistical modeling or machine learning applications is converting a categorical variable into a dummy or indicator matrix, also known as one-hot encoding. \n",
    "If a column in a DataFrame has `k` distinct values, you would derive a matrix or DataFrame with `k` columns containing all 1s and 0s. pandas has a `pandas.get_dummies` function for doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22652aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example de DF:\n",
    "df = pd.DataFrame({\"key\": [\"b\", \"b\", \"a\", \"c\", \"a\", \"b\"],\n",
    "                       \"data1\": range(6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5949be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51029712",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df[\"key\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c84b80",
   "metadata": {},
   "source": [
    "If a row in a DataFrame belongs to multiple categories, we have to use a different approach to create the dummy variables. For example, in the MovieLens 1M dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253feb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnames = [\"movie_id\", \"title\", \"genres\"]\n",
    "movies = pd.read_table(\"movies.dat\", sep=\"::\",\n",
    "                           header=None, names=mnames, engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f6b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbe9b5",
   "metadata": {},
   "source": [
    "pandas has a special Series method `str.get_dummies` that handles multiple group membership encoded as a delimited string, in this case, `|` for the genres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = movies[\"genres\"].str.get_dummies(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6daefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies.iloc[:10, :6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94533f3d",
   "metadata": {},
   "source": [
    "### 2.3 Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d7f51",
   "metadata": {},
   "source": [
    "Suppose that you have a dataset with sentences like comments in a social media. A way to represent text as numeric data is using one-hot encoding using the words as categories. However, there are another approaches that could yield to better performance when training a Machine Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d90fa",
   "metadata": {},
   "source": [
    "#### Bag Of Words\n",
    "The first alternative approach to one-hot-encoding is the Bag of Words (BoW) model. Suppose you have the next toy dataset with 4 sentences:\n",
    "\n",
    "`doc = ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']`\n",
    "\n",
    "If we map the words with IDs as: dog = 1, bites = 2, man = 3, meat = 4 , food = 5, eats = 6\n",
    "Then the one-hot encoding scheme for the first sentence would be\n",
    "\n",
    "`[[1 0 0 0 0 0], [0 1 0 0 0 0], [0 0 1 0 0 0]]`\n",
    "\n",
    "The BoW model counts the frequencies of words in a sentence assigning the total of counts to the IDs. Thus, in the BoW model, the first sentence representantion can be stated as:\n",
    "\n",
    "`[1 1 1 0 0 0]`.\n",
    "\n",
    "This is because the first three words in the vocabulary appeared exactly once, and the last three did not appear at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e752ff1",
   "metadata": {},
   "source": [
    "#### scikit-learn API\n",
    "In this and the next section we are going to use the sklearn package for the transformation of data, namely, text representation and scalings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc48b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02e5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a BOW representation for the corpus\n",
    "bow_rep = count_vect.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024362cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at the vocabulary mapping\n",
    "print(\"Our vocabulary: \", count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa82e717",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the BOW rep for first 2 documents\n",
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2fdcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the representation using this vocabulary, for a new text\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\",\n",
    "temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00525803",
   "metadata": {},
   "source": [
    "Notice that 'dog' is the only word considered when counting because of its presence in the original voabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d65f55",
   "metadata": {},
   "source": [
    "Sometimes, we don’t care about the frequency of occurrence of words in text and we only want to represent whether a word exists in the text or not. In this case, use the `binary` argument set to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0af8866",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect.fit_transform(doc)\n",
    "temp = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70371e6",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614ee851",
   "metadata": {},
   "source": [
    "`TF-IDF`, or term frequency–inverse document frequency, quantify the importance of a given word relative to other words in the document and in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb55f57c",
   "metadata": {},
   "source": [
    "`TF` (term frequency) measures how often a term or word occurs in a given document. Since different documents in the corpus may be of different lengths, a term may occur more often in a longer document as compared to a shorter document. To normalize these counts, we divide the number of occurrences by the length of the document. `IDF` (inverse document frequency) measures the importance of the term across a corpus. This solves the problem of common stop words like, is, are, am, etc. Now let's see a TF-IDF implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1828c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26589e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "bow_rep_tfidf = tfidf.fit_transform(doc)\n",
    "print(tfidf.idf_) #IDF for all words in the vocabulary\n",
    "print(tfidf.get_feature_names()) #All words in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tfidf.transform([\"dog and man are friends\"])\n",
    "print(\"Tfidf representation for 'dog and man are friends':\\n\", temp.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d85b7",
   "metadata": {},
   "source": [
    "### 2.4 Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17897788",
   "metadata": {},
   "source": [
    "Let's assume that we have two features where one feature is measured on a scale from 1 to 10 and the second feature is measured on a scale from 1 to 100,000, respectively. Some ML algorithms would fail to converge or would too long to converge. Now, there are two common approaches to bringing different features onto the same\n",
    "scale: normalization and standardization. Normalization refers to the rescaling of the features to a range of `[0, 1]`, which is a special case of min-max scaling.\n",
    "\n",
    "Using standardization, we center the feature columns at mean 0 with standard deviation 1 so that the feature  columns have the same parameters as a standard normal distribution (zero mean and unit variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4745cffc",
   "metadata": {},
   "source": [
    "We illustrate whis with the popular iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a25b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38df17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b077d",
   "metadata": {},
   "source": [
    "Let's use the sklearn MinMaxScaler for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1155c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1507b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mms = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea1ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = mms.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d225c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3081d025",
   "metadata": {},
   "source": [
    "Similar to the MinMaxScaler, let's use the sklearn StandardScaler for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc230ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacdbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdsc = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3feae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = stdsc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd81d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0d027",
   "metadata": {},
   "source": [
    "## 3. Extra Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41270731",
   "metadata": {},
   "source": [
    "### 3.1 sklearn tranformation pipelines\n",
    "#### Suggested Reading: https://scikit-learn.org/stable/modules/compose.html#pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93299a4",
   "metadata": {},
   "source": [
    "### 3.2 Feature engineering\n",
    "#### Suggested Reading: https://www.analyticsvidhya.com/blog/2021/03/step-by-step-process-of-feature-engineering-for-machine-learning-algorithms-in-data-science/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
